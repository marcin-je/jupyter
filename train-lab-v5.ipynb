{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e22380",
   "metadata": {},
   "source": [
    "0. Load all data\n",
    "1. Define numerics column\n",
    "2. Load PCA and FA from pickle\n",
    "3. Run PCA and FA over numerics\n",
    "4. Append to existing DF\n",
    "5. Cross-check columns in DF with selected ones\n",
    "6. Remove extra columns\n",
    "7. Split into train, test, validate\n",
    "8. May need to run stratify for fold creation\n",
    "9. Define early stopping func\n",
    "10. Re-train on best best_n_tree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00e5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "from featureeng.times import get_trading_times_for_london_ny\n",
    "from featureeng.features import *\n",
    "import featureeng as feng\n",
    "import data.readers as rd\n",
    "import joblib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "from functools import partial\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Classifier/Regressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Feature selection\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "# Data processing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import pickle\n",
    "# Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#helper functions\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from featureeng.times import get_trading_times_for_london_ny\n",
    "from featureeng.features import *\n",
    "import featureeng as feng\n",
    "import data.readers as rd\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "from functools import partial\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Feature selection\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "# Data processing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "root_dir = '/home/dcai/data/features'\n",
    "instrument='EURCHF'\n",
    "feature_set=\"3\"\n",
    "\n",
    "\n",
    "\n",
    "temp_save_dir=\"{}/{}\".format(root_dir,instrument)\n",
    "\n",
    "def save_df(save_dir, df, name):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    pq.write_table(pa.Table.from_pandas(df), os.path.join(save_dir, name))\n",
    "\n",
    "\n",
    "temp_save_dir=\"{}/{}\".format(root_dir,instrument)\n",
    "def read_parquet(save_dir, name):\n",
    "    return pq.ParquetFile(os.path.join(save_dir, name)).read().to_pandas()\n",
    "\n",
    "\n",
    "\n",
    "def list_of_files(root_dir):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            # Check if the file is a CSV file\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "\n",
    "def read_in_files(file_list):\n",
    "    dfs = []\n",
    "    for f in file_list:\n",
    "        dfs.append(pq.ParquetFile(f).read().to_pandas())\n",
    "    return pd.concat(dfs).sort_index()\n",
    "\n",
    "def train_valid_test(instrument, feature_set, root_dir='/home/dcai/data/features',  train_size=0.75):\n",
    "    files = list_of_files(\"{}/{}/{}\".format(root_dir,instrument, feature_set))\n",
    "    files.sort()\n",
    "\n",
    "    train_index = int(len(files)*train_size)\n",
    "    train = files[0:train_index]\n",
    "    valid_test = files[train_index:]\n",
    "\n",
    "\n",
    "    valid = valid_test[0::2]\n",
    "    test = valid_test[1::2]\n",
    "\n",
    "    return {'train': read_in_files(train),\n",
    "            'valid': read_in_files(valid),\n",
    "            'test': read_in_files(test)\n",
    "           }\n",
    "\n",
    "def extract_features_from_gmm_cols(gmm_cols):\n",
    "    result = [splits[0] for string in gmm_cols for splits in [string.split(\"_gmm_\")] if len(splits) > 1]\n",
    "    return list(set(result))\n",
    "\n",
    "\n",
    "def save_df(save_dir, df, name):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    pq.write_table(pa.Table.from_pandas(df), os.path.join(save_dir, name))\n",
    "\n",
    "\n",
    "\n",
    "def read_parquet(save_dir, name):\n",
    "    return pq.ParquetFile(os.path.join(save_dir, name)).read().to_pandas()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#allow logloss and classification error plots for each iteraetion of xgb model\n",
    "def plot_compare(metrics,eval_results,epochs):\n",
    "    for m in metrics:\n",
    "        test_score = eval_results['val'][m]\n",
    "        train_score = eval_results['train'][m]\n",
    "        rang = range(0, epochs)\n",
    "        plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "        plt.plot(rang, test_score,\"c\", label=\"Val\")\n",
    "        plt.plot(rang, train_score,\"orange\", label=\"Train\")\n",
    "        title_name = m + \" plot\"\n",
    "        plt.title(title_name)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel(m)\n",
    "        lgd = plt.legend()\n",
    "        plt.show()    \n",
    "    \n",
    "#pre_selected_cols = ['MeanPrice_x', 'close_MIN_1D_5', 'close_MIN_1D_20', 'close_MIN_1D_60', 'close_MAX_1D_5', 'close_MAX_1D_20', 'close_MAX_1D_60', 'close_AVG_1D_5', 'close_AVG_1D_20', 'close_AVG_1D_60', 'close_STDEV_1D_5', 'close_STDEV_1D_20', 'close_STDEV_1D_60', 'close_MEDIAN_1D_5', 'close_MEDIAN_1D_20', 'close_MEDIAN_1D_60', 'close_MIN_1H_5', 'close_MIN_1H_20', 'close_MIN_1H_60', 'close_MAX_1H_5', 'close_MAX_1H_20', 'close_MAX_1H_60', 'close_AVG_1H_5', 'close_AVG_1H_20', 'close_AVG_1H_60', 'close_STDEV_1H_5', 'close_STDEV_1H_20', 'close_STDEV_1H_60', 'close_MEDIAN_1H_5', 'close_MEDIAN_1H_20', 'close_MEDIAN_1H_60', 'London', 'NY', 'rolling_1D_60_3_1', 'rolling_1D_60_3_2', 'rolling_1D_60_3_3', 'ewm_1D_60_3_1', 'ewm_1D_60_3_2', 'ewm_1D_60_3_3', 'rolling_1D_20_4_1', 'rolling_1D_20_4_2', 'rolling_1D_20_4_3', 'rolling_1D_20_4_4', 'ewm_1D_20_4_1', 'ewm_1D_20_4_2', 'ewm_1D_20_4_3', 'ewm_1D_20_4_4', 'rolling_4H_20_4_1', 'rolling_4H_20_4_2', 'rolling_4H_20_4_3', 'rolling_4H_20_4_4', 'ewm_4H_20_4_1', 'ewm_4H_20_4_2', 'ewm_4H_20_4_3', 'ewm_4H_20_4_4', 'rolling_1D_60_20_4_1', 'rolling_1D_60_20_4_2', 'rolling_1D_60_20_4_3', 'rolling_1D_60_20_4_4', 'rolling_4H_60_20_4_1', 'rolling_4H_60_20_4_2', 'rolling_4H_60_20_4_3', 'rolling_4H_60_20_4_4', 'MeanPrice_y', 'EUR', 'CHF', 'USD', 'GBP', 'fa_0', 'fa_1', 'fa_2', 'fa_3', 'fa_4', 'fa_5', 'fa_6', 'fa_7', 'fa_8', 'fa_9', 'fa_10', 'fa_11', 'fa_12', 'fa_13', 'fa_14', 'fa_15', 'fa_16', 'fa_17', 'fa_18', 'fa_19', 'fa_20', 'fa_21', 'fa_22', 'fa_23', 'fa_24', 'fa_25', 'fa_26', 'fa_27', 'fa_28', 'fa_29', 'fa_30', 'fa_31', 'fa_32', 'fa_33', 'fa_34', 'fa_35', 'fa_36', 'fa_37', 'fa_38', 'fa_39', 'fa_40', 'fa_41', 'fa_42', 'fa_43', 'fa_44', 'fa_45', 'fa_46', 'fa_47', 'fa_48', 'fa_49', 'fa_50', 'fa_51', 'fa_52', 'fa_53', 'fa_54', 'fa_55', 'fa_56', 'fa_57', 'fa_58', 'fa_59', 'fa_60', 'fa_61', 'fa_62', 'fa_63', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14', 'pca_15', 'pca_16', 'pca_17', 'pca_18', 'pca_19', 'pca_20', 'pca_21', 'pca_22', 'pca_23', 'pca_24', 'pca_25', 'pca_26', 'pca_27', 'pca_28', 'pca_29', 'pca_30', 'pca_31', 'pca_32', 'pca_33', 'pca_34', 'pca_35', 'pca_36', 'pca_37', 'pca_38', 'pca_39', 'pca_40', 'pca_41', 'pca_42', 'pca_43', 'pca_44', 'pca_45', 'pca_46', 'pca_47', 'pca_48', 'pca_49', 'pca_50', 'pca_51', 'pca_52', 'pca_53', 'pca_54', 'pca_55', 'pca_56', 'pca_57', 'pca_58', 'pca_59', 'pca_60', 'pca_61', 'pca_62', 'pca_63']\n",
    "#X_train = read_parquet(temp_save_dir, 'X_train.parquet')#[pre_selected_cols]\n",
    "#Y_train = read_parquet(temp_save_dir, 'Y_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69493ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 11.863 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()    \n",
    "result = train_valid_test(instrument=instrument, feature_set=feature_set)\n",
    "end_time = time()\n",
    "print(f'Execution time: {end_time - start_time:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413ceb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901bab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerics = ['MeanPrice', 'close_MIN_1D_5', 'close_MIN_1D_20', 'close_MIN_1D_60',\n",
    "       'close_MAX_1D_5', 'close_MAX_1D_20', 'close_MAX_1D_60',\n",
    "       'close_AVG_1D_5', 'close_AVG_1D_20', 'close_AVG_1D_60',\n",
    "       'close_STDEV_1D_5', 'close_STDEV_1D_20', 'close_STDEV_1D_60',\n",
    "       'close_MEDIAN_1D_5', 'close_MEDIAN_1D_20', 'close_MEDIAN_1D_60',\n",
    "       'close_MIN_1H_5', 'close_MIN_1H_20', 'close_MIN_1H_60',\n",
    "       'close_MAX_1H_5', 'close_MAX_1H_20', 'close_MAX_1H_60',\n",
    "       'close_AVG_1H_5', 'close_AVG_1H_20', 'close_AVG_1H_60',\n",
    "       'close_STDEV_1H_5', 'close_STDEV_1H_20', 'close_STDEV_1H_60',\n",
    "       'close_MEDIAN_1H_5', 'close_MEDIAN_1H_20', 'close_MEDIAN_1H_60',\n",
    "       'rolling_1D_60_3_1', 'rolling_1D_60_3_2',\n",
    "       'rolling_1D_60_3_3', 'ewm_1D_60_3_1', 'ewm_1D_60_3_2', 'ewm_1D_60_3_3',\n",
    "       'rolling_1D_20_4_1', 'rolling_1D_20_4_2', 'rolling_1D_20_4_3',\n",
    "       'rolling_1D_20_4_4', 'ewm_1D_20_4_1', 'ewm_1D_20_4_2', 'ewm_1D_20_4_3',\n",
    "       'ewm_1D_20_4_4', 'rolling_4H_20_4_1', 'rolling_4H_20_4_2',\n",
    "       'rolling_4H_20_4_3', 'rolling_4H_20_4_4', 'ewm_4H_20_4_1',\n",
    "       'ewm_4H_20_4_2', 'ewm_4H_20_4_3', 'ewm_4H_20_4_4',\n",
    "       'rolling_1D_60_20_4_1', 'rolling_1D_60_20_4_2', 'rolling_1D_60_20_4_3',\n",
    "       'rolling_1D_60_20_4_4', 'rolling_4H_60_20_4_1', 'rolling_4H_60_20_4_2',\n",
    "       'rolling_4H_60_20_4_3', 'rolling_4H_60_20_4_4']\n",
    "\n",
    "labels = [ 'label_60_5_buy', 'label_60_5_sell', 'label_60_5_stay']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = result['train']\n",
    "test = result['test']\n",
    "valid = result['valid']\n",
    "\n",
    "X_train = train[numerics]\n",
    "Y_train = train[labels]\n",
    "\n",
    "X_test = test[numerics]\n",
    "Y_test = test[labels]\n",
    "\n",
    "X_valid = valid[numerics]\n",
    "Y_valid = valid[labels]\n",
    "\n",
    "result = None\n",
    "train = None\n",
    "test = None\n",
    "valid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8214aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcai/data/features/EURCHF/fa.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(os.path.join(temp_save_dir, 'fa.pickle'))\n",
    "\n",
    "with open(os.path.join(temp_save_dir, 'fa.pickle'), 'rb') as f:\n",
    "    fa = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(temp_save_dir, 'pca.pickle'), 'rb') as f:\n",
    "    pca = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(temp_save_dir, 'km.pickle'), 'rb') as f:\n",
    "    km = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(temp_save_dir, 'y_stratified.pickle'), 'rb') as f:\n",
    "    y_stratified = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(temp_save_dir, 'selected_feats-5.pickle'), 'rb') as f:\n",
    "    selected_feats = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b5812f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_feats = [f'fa_{i}'for i in range(len(numerics))]#[:4]\n",
    "\n",
    "X_train[fa_feats] = fa.transform(X_train[numerics])\n",
    "X_test[fa_feats] = fa.transform(X_test[numerics])\n",
    "X_valid[fa_feats] = fa.transform(X_valid[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d801d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feats = [f'pca_{i}'for i in range(len(numerics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feats_raw = pca.transform(X_train[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57b943b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13080771, 61)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[numerics].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579fb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[pca_feats] = pca.transform(X_train[numerics])\n",
    "X_test[pca_feats] = pca.transform(X_test[numerics])\n",
    "X_valid[pca_feats] = pca.transform(X_valid[numerics])\n",
    "#valid[pca_feats] = pca.transform(valid[numerics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6705eb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MeanPrice', 'close_MIN_1D_5', 'close_MIN_1D_20',\n",
       "       'close_MIN_1D_60', 'close_MAX_1D_5', 'close_MAX_1D_20',\n",
       "       'close_MAX_1D_60', 'close_AVG_1D_5', 'close_AVG_1D_20',\n",
       "       'close_AVG_1D_60', 'close_STDEV_1D_5', 'close_STDEV_1D_20',\n",
       "       'close_STDEV_1D_60', 'close_MEDIAN_1D_5', 'close_MEDIAN_1D_20',\n",
       "       'close_MEDIAN_1D_60', 'close_MIN_1H_5', 'close_MIN_1H_20',\n",
       "       'close_MIN_1H_60', 'close_MAX_1H_5', 'close_MAX_1H_20',\n",
       "       'close_MAX_1H_60', 'close_AVG_1H_5', 'close_AVG_1H_20',\n",
       "       'close_AVG_1H_60', 'close_STDEV_1H_5', 'close_STDEV_1H_20',\n",
       "       'close_STDEV_1H_60', 'close_MEDIAN_1H_5', 'close_MEDIAN_1H_20',\n",
       "       'close_MEDIAN_1H_60', 'rolling_1D_60_3_1', 'rolling_1D_60_3_2',\n",
       "       'rolling_1D_60_3_3', 'ewm_1D_60_3_1', 'ewm_1D_60_3_2',\n",
       "       'ewm_1D_60_3_3', 'rolling_1D_20_4_1', 'rolling_1D_20_4_2',\n",
       "       'rolling_1D_20_4_3', 'rolling_1D_20_4_4', 'ewm_1D_20_4_1',\n",
       "       'ewm_1D_20_4_2', 'ewm_1D_20_4_3', 'ewm_1D_20_4_4',\n",
       "       'rolling_4H_20_4_1', 'rolling_4H_20_4_2', 'rolling_4H_20_4_3',\n",
       "       'rolling_4H_20_4_4', 'ewm_4H_20_4_1', 'ewm_4H_20_4_2',\n",
       "       'ewm_4H_20_4_3', 'ewm_4H_20_4_4', 'rolling_1D_60_20_4_1',\n",
       "       'rolling_1D_60_20_4_2', 'rolling_1D_60_20_4_3',\n",
       "       'rolling_1D_60_20_4_4', 'rolling_4H_60_20_4_1',\n",
       "       'rolling_4H_60_20_4_2', 'rolling_4H_60_20_4_3',\n",
       "       'rolling_4H_60_20_4_4', 'fa_0', 'fa_1', 'fa_2', 'fa_3', 'fa_4',\n",
       "       'fa_5', 'fa_6', 'fa_7', 'fa_8', 'fa_9', 'fa_10', 'fa_11', 'fa_12',\n",
       "       'fa_13', 'fa_14', 'fa_15', 'fa_16', 'fa_17', 'fa_18', 'fa_19',\n",
       "       'fa_20', 'fa_21', 'fa_22', 'fa_23', 'fa_24', 'fa_25', 'fa_26',\n",
       "       'fa_27', 'fa_28', 'fa_29', 'fa_30', 'fa_31', 'fa_32', 'fa_33',\n",
       "       'fa_34', 'fa_35', 'fa_36', 'fa_37', 'fa_38', 'fa_39', 'fa_40',\n",
       "       'fa_41', 'fa_42', 'fa_43', 'fa_44', 'fa_45', 'fa_46', 'fa_47',\n",
       "       'fa_48', 'fa_49', 'fa_50', 'fa_51', 'fa_52', 'fa_53', 'fa_54',\n",
       "       'fa_55', 'fa_56', 'fa_57', 'fa_58', 'fa_59', 'fa_60', 'pca_0',\n",
       "       'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7',\n",
       "       'pca_8', 'pca_9', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14',\n",
       "       'pca_15', 'pca_16', 'pca_17', 'pca_18', 'pca_19', 'pca_20',\n",
       "       'pca_21', 'pca_22', 'pca_23', 'pca_24', 'pca_25', 'pca_26',\n",
       "       'pca_27', 'pca_28', 'pca_29', 'pca_30', 'pca_31', 'pca_32',\n",
       "       'pca_33', 'pca_34', 'pca_35', 'pca_36', 'pca_37', 'pca_38',\n",
       "       'pca_39', 'pca_40', 'pca_41', 'pca_42', 'pca_43', 'pca_44',\n",
       "       'pca_45', 'pca_46', 'pca_47', 'pca_48', 'pca_49', 'pca_50',\n",
       "       'pca_51', 'pca_52', 'pca_53', 'pca_54', 'pca_55', 'pca_56',\n",
       "       'pca_57', 'pca_58', 'pca_59', 'pca_60'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e992f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "-----\n",
      "['MeanPrice', 'close_AVG_1D_20', 'close_AVG_1D_5', 'close_AVG_1D_60', 'close_AVG_1H_20', 'close_AVG_1H_5', 'close_AVG_1H_60', 'close_MAX_1D_20', 'close_MAX_1D_5', 'close_MAX_1D_60', 'close_MAX_1H_20', 'close_MAX_1H_5', 'close_MAX_1H_60', 'close_MEDIAN_1D_20', 'close_MEDIAN_1D_5', 'close_MEDIAN_1D_60', 'close_MEDIAN_1H_20', 'close_MEDIAN_1H_5', 'close_MEDIAN_1H_60', 'close_MIN_1D_20', 'close_MIN_1D_5', 'close_MIN_1D_60', 'close_MIN_1H_20', 'close_MIN_1H_5', 'close_MIN_1H_60', 'close_STDEV_1D_20', 'close_STDEV_1D_5', 'close_STDEV_1D_60', 'close_STDEV_1H_20', 'close_STDEV_1H_5', 'close_STDEV_1H_60', 'ewm_1D_20_4_1', 'ewm_1D_20_4_2', 'ewm_1D_20_4_3', 'ewm_1D_20_4_4', 'ewm_1D_60_3_1', 'ewm_1D_60_3_2', 'ewm_1D_60_3_3', 'ewm_4H_20_4_1', 'ewm_4H_20_4_2', 'ewm_4H_20_4_3', 'ewm_4H_20_4_4', 'fa_0', 'fa_1', 'fa_10', 'fa_11', 'fa_12', 'fa_2', 'fa_3', 'fa_4', 'fa_5', 'fa_6', 'fa_7', 'fa_8', 'fa_9', 'pca_0', 'pca_1', 'pca_10', 'pca_11', 'pca_12', 'pca_13', 'pca_14', 'pca_15', 'pca_16', 'pca_17', 'pca_18', 'pca_19', 'pca_2', 'pca_20', 'pca_21', 'pca_22', 'pca_23', 'pca_24', 'pca_25', 'pca_26', 'pca_27', 'pca_28', 'pca_29', 'pca_3', 'pca_30', 'pca_31', 'pca_32', 'pca_33', 'pca_34', 'pca_35', 'pca_36', 'pca_37', 'pca_38', 'pca_39', 'pca_4', 'pca_40', 'pca_41', 'pca_42', 'pca_43', 'pca_44', 'pca_45', 'pca_46', 'pca_47', 'pca_48', 'pca_49', 'pca_5', 'pca_50', 'pca_51', 'pca_52', 'pca_53', 'pca_54', 'pca_55', 'pca_56', 'pca_58', 'pca_59', 'pca_6', 'pca_60', 'pca_7', 'pca_8', 'pca_9', 'rolling_1D_20_4_1', 'rolling_1D_20_4_2', 'rolling_1D_20_4_3', 'rolling_1D_20_4_4', 'rolling_1D_60_20_4_1', 'rolling_1D_60_20_4_2', 'rolling_1D_60_20_4_3', 'rolling_1D_60_20_4_4', 'rolling_1D_60_3_1', 'rolling_1D_60_3_2', 'rolling_1D_60_3_3', 'rolling_4H_20_4_1', 'rolling_4H_20_4_2', 'rolling_4H_20_4_3', 'rolling_4H_20_4_4', 'rolling_4H_60_20_4_1', 'rolling_4H_60_20_4_2', 'rolling_4H_60_20_4_3', 'rolling_4H_60_20_4_4']\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "any_missing = list(set(selected_feats)-set(X_train.columns.values))\n",
    "print(any_missing)\n",
    "print(\"-----\")\n",
    "selected_feats_3 = list(set(selected_feats)-set(any_missing))\n",
    "\n",
    "selected_feats_3.sort()\n",
    "\n",
    "#selected_feats_3 = [feat for feat in selected_feats_3 if not \"gmm\" in feat]\n",
    "print(selected_feats_3)\n",
    "\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0f996c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13080771, 134)\n",
      "(13080771,)\n",
      "(2168466, 134)\n",
      "(2168466,)\n",
      "(2114119, 134)\n",
      "(2114119,)\n"
     ]
    }
   ],
   "source": [
    "trainX = X_train[selected_feats_3]\n",
    "trainY = Y_train[labels].values.argmax(axis=1)\n",
    "\n",
    "validateX = X_valid[selected_feats_3]\n",
    "validateY = Y_valid[labels].values.argmax(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "testX = X_test[selected_feats_3]\n",
    "testY = Y_test[labels].values.argmax(axis=1)\n",
    "\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "print(validateX.shape)\n",
    "print(validateY.shape)\n",
    "\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ed0c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(temp_save_dir, trainX, \"trainX.parqet\")\n",
    "\n",
    "save_df(temp_save_dir, validateX, \"validateX.parqet\")\n",
    "\n",
    "save_df(temp_save_dir, testX, \"testX.parqet\")\n",
    "\n",
    "\n",
    "with open(os.path.join(temp_save_dir, 'trainY.pickle'), 'wb') as f:\n",
    "    pickle.dump(trainY, f)\n",
    "    \n",
    "with open(os.path.join(temp_save_dir, 'validateY.pickle'), 'wb') as f:\n",
    "    pickle.dump(validateY, f)\n",
    "    \n",
    "with open(os.path.join(temp_save_dir, 'testY.pickle'), 'wb') as f:\n",
    "    pickle.dump(testY, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "019d9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier, DMatrix\n",
    "\n",
    "dtrain = DMatrix(trainX, trainY, enable_categorical=False)\n",
    "dvalid =  DMatrix(validateX, label=validateY, enable_categorical=False)\n",
    "dtest = DMatrix(testX, label=testY, enable_categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9acee503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of combinations:             5760\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "## parameters to be tuned\n",
    "tune_dic = OrderedDict()\n",
    "\n",
    "tune_dic['max_depth']= [5,6,7,8,10,15,20,25] ## maximum tree depth\n",
    "tune_dic['subsample']=[0.5,0.6,0.7,0.8,0.9,1.0] ## proportion of training instances used in trees\n",
    "tune_dic['colsample_bytree']= [0.5,0.6,0.7,0.8,0.9,1.0] ## subsample ratio of columns\n",
    "tune_dic['eta']= [0.01,0.25,0.05,0.10,]  ## learning rate\n",
    "tune_dic['gamma']= [0.00,0.05,0.10,0.15,0.20]  ## minimum loss function reduction required for a split\n",
    "#tune_dic['scale_pos_weight']=[30,40,50,300,400,500,600,700] ## relative weight of positive/negative instances\n",
    "\n",
    "lengths = [len(lst) for lst in tune_dic.values()]\n",
    "\n",
    "combs=1\n",
    "for i in range(len(lengths)):\n",
    "    combs *= lengths[i]\n",
    "print('Total number of combinations: {:16d}'.format(combs))  \n",
    "\n",
    "maxiter=500\n",
    "\n",
    "columns=[*tune_dic.keys()]+['F-Score','Best F-Score']\n",
    "results = pd.DataFrame(index=range(maxiter), columns=columns) ## dataframe to hold training results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "734c54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_choice(cur_params=None):\n",
    "    ## returns a random combination of the variable parameters (if cur_params=None)\n",
    "    ## or a random neighboring combination from cur_params\n",
    "    if cur_params:\n",
    "        ## chose parameter to change\n",
    "        ## parameter name and current value\n",
    "        choose_param_name, cur_value = random.choice(list(cur_choice.items())) ## parameter name \n",
    "       \n",
    "        all_values =  list(tune_dic[choose_param_name]) ## all values of selected parameter\n",
    "        cur_index = all_values.index(cur_value) ## current index of selected parameter\n",
    "        \n",
    "        if cur_index==0: ## if it is the first in the range select the second one\n",
    "            next_index=1\n",
    "        elif cur_index==len(all_values)-1: ## if it is the last in the range select the previous one\n",
    "            next_index=len(all_values)-2\n",
    "        else: ## otherwise select the left or right value randomly\n",
    "            direction=np.random.choice([-1,1])\n",
    "            next_index=cur_index + direction\n",
    "\n",
    "        next_params = dict((k,v) for k,v in cur_params.items())\n",
    "        next_params[choose_param_name] = all_values[next_index] ## change the value of the selected parameter\n",
    "        print('selected move: {:10s}: from {:6.2f} to {:6.2f}'.\n",
    "              format(choose_param_name, cur_value, all_values[next_index] ))\n",
    "    else: ## generate a random combination of parameters\n",
    "        next_params=dict()\n",
    "        for i in range(len(tune_dic)):\n",
    "            key = [*tune_dic.keys()][i] \n",
    "            values = [*tune_dic.values()][i]\n",
    "            next_params[key] = np.random.choice(values)\n",
    "    return(next_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
