{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd36d2e-e7c3-4b10-84a9-1986dd6bec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 1.5.3\n"
     ]
    }
   ],
   "source": [
    "#import cudf as pd #Change1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "print('We will use RAPIDS version',pd.__version__)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "train_model = True\n",
    "model_to_restore = \"model_cnn_1.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4114a25d-e332-4c1c-abe1-b3c86e697769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stock_id', 'seconds_in_bucket', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "features = [col for col in train.columns if col not in ['row_id', 'time_id', 'date_id', 'target']]\n",
    "print(features)\n",
    "print(len(features))\n",
    "\n",
    "train['bid_size_not_norm']=train['bid_size']\n",
    "train['ask_size_not_norm']=train['ask_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28ccd24d-5f8b-4475-8a9d-d33f4dfbf184",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_id_ranges = {\n",
    "    stock_id: [group['target'].min(), group['target'].max()]\n",
    "    for stock_id, group in train.groupby('stock_id')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3c5751-97b1-42be-9159-3054c4128103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                    0\n",
       "date_id                     0\n",
       "seconds_in_bucket           0\n",
       "imbalance_size              0\n",
       "imbalance_buy_sell_flag     0\n",
       "reference_price             0\n",
       "matched_size                0\n",
       "far_price                   0\n",
       "near_price                  0\n",
       "bid_price                   0\n",
       "bid_size                    0\n",
       "ask_price                   0\n",
       "ask_size                    0\n",
       "wap                         0\n",
       "target                     88\n",
       "time_id                     0\n",
       "row_id                      0\n",
       "bid_size_not_norm           0\n",
       "ask_size_not_norm           0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop dataset size: 5237980\n",
      "after drop dataset size: 5237892\n"
     ]
    }
   ],
   "source": [
    "train['far_price'].fillna(0, inplace=True)\n",
    "train['near_price'].fillna(1, inplace=True)\n",
    "\n",
    "cols_group_by = ['date_id', 'seconds_in_bucket']\n",
    "cat_cols = ['stock_id', 'imbalance_buy_sell_flag']\n",
    "cols_fill_nan = [\n",
    "    'imbalance_size', 'reference_price', 'matched_size', 'wap',\n",
    "    'bid_price', 'bid_size', 'ask_price', 'ask_size', \n",
    "    'stock_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag', 'bid_size_not_norm', 'ask_size_not_norm']\n",
    "train_grouped_median = train.groupby(cols_group_by)[cols_fill_nan].transform('median')\n",
    "\n",
    "train[cols_fill_nan] = train[cols_fill_nan].fillna(train_grouped_median)\n",
    "\n",
    "display(train.isnull().sum())\n",
    "print(f\"before drop dataset size: {len(train)}\")\n",
    "train.dropna(inplace=True)\n",
    "print(f\"after drop dataset size: {len(train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a497949-d313-47b6-89f9-c397a7c72515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "\n",
    "num_cols = [feature for feature in features if feature not in cat_cols]\n",
    "print(len(num_cols))\n",
    "\n",
    "train[num_cols] = train[num_cols].astype('float32')\n",
    "if train_model:\n",
    "    robust_scaler = QuantileTransformer(output_distribution='normal')\n",
    "    train[num_cols] = robust_scaler.fit_transform(train[num_cols])\n",
    "    joblib.dump(robust_scaler, 'quantile_transformer.pkl')\n",
    "else:\n",
    "    #robust_scaler = joblib.load('robust_scaler.pkl')\n",
    "    robust_scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "    train[num_cols] = robust_scaler.fit_transform(train[num_cols])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f85a93f-02ad-47c7-bddb-431e29b16ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§¹ Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ðŸ“ Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # ðŸ”„ Iterate through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # â„¹ï¸ Provide memory optimization information if 'verbose' is True\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # ðŸ”„ Return the DataFrame with optimized memory usage\n",
    "    return df\n",
    "\n",
    "# ðŸŽï¸ Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# ðŸ“Š Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # ðŸ” Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # ðŸ” Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # ðŸš« Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# ðŸ“ˆ Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "# ðŸ“Š Function to generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    import cudf\n",
    "    df = cudf.from_pandas(df)\n",
    "    \n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1 features\n",
    "    # Calculate various features using Pandas eval function\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    # V2 features\n",
    "    # Calculate additional features\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    \n",
    "        \n",
    "    # V3 features\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    df = df.to_pandas()\n",
    "    # Replace infinite values with 0\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def numba_imb_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    return df\n",
    "\n",
    "# ðŸ“… Function to generate time and stock-related features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n",
    "\n",
    "    # Map global features to the DataFrame\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# ðŸš€ Function to generate all features by combining imbalance and other features\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = numba_imb_features(df)\n",
    "    # Generate time and stock-related features\n",
    "    df = other_features(df)\n",
    "    gc.collect()  # Perform garbage collection to free up memory\n",
    "    \n",
    "    # Select and return the generated features\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3681bab-cc81-412a-9e9d-0bb8e5606aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439c5b1c-d6da-4eff-8cb2-3e7f1c5e49c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding stock_id ... Done\n",
      "encoding imbalance_buy_sell_flag ... Done\n"
     ]
    }
   ],
   "source": [
    "def encode(encoder, x):\n",
    "    len_encoder = len(encoder)\n",
    "    try:\n",
    "        id = encoder[x]\n",
    "    except KeyError:\n",
    "        id = len_encoder\n",
    "    return id\n",
    "\n",
    "encoders = [{} for cat in cat_cols]\n",
    "\n",
    "\n",
    "for i, cat in enumerate(cat_cols):\n",
    "    print('encoding %s ...' % cat, end=' ')\n",
    "    encoders[i] = {l: id for id, l in enumerate(train.loc[:, cat].astype(str).unique())}\n",
    "    train[cat] = train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "embed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d49e8b-269a-449c-99b3-aa65f7c14315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[201, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25700591-d464-47c1-84a6-ca2b540f0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = train[features].copy(deep=True)\n",
    "#y_train = train['target'].copy(deep=True)\n",
    "\n",
    "#X_train.fillna(0, inplace = True)\n",
    "#y_train.fillna(0, inplace = True)\n",
    "\n",
    "\n",
    "X = train.drop(columns=['target'])  # Your features\n",
    "#X['stock_id'] = X['stock_id'].astype(\"category\")\n",
    "y = train['target']  # Your target variable\n",
    "groups = train['date_id'] # Extracting just the 'time_id' column for grouping\n",
    "\n",
    "groups.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#train.fillna(0, inplace = True)\n",
    "#X.fillna(0, inplace = True)\n",
    "#y.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be9c8b-ce8f-411d-aa0d-b644a83a7f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55bf993-d327-495f-9230-335be5acb0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:14:22.461723: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-22 20:14:22.477116: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 20:14:22.477137: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 20:14:22.477150: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 20:14:22.480503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________________\n",
      " Layer (type)             Output Shape              Param    Connected to               \n",
      "                                                    #                                   \n",
      "========================================================================================\n",
      " stock_id (InputLayer)    [(None, 1)]               0        []                         \n",
      "                                                                                        \n",
      " imbalance_buy_sell_flag  [(None, 1)]               0        []                         \n",
      "  (InputLayer)                                                                          \n",
      "                                                                                        \n",
      " embedding (Embedding)    (None, 1, 15)             3015     ['stock_id[0][0]']         \n",
      "                                                                                        \n",
      " embedding_1 (Embedding)  (None, 1, 3)              12       ['imbalance_buy_sell_flag[0\n",
      "                                                             ][0]']                     \n",
      "                                                                                        \n",
      " num (InputLayer)         [(None, 11)]              0        []                         \n",
      "                                                                                        \n",
      " flatten (Flatten)        (None, 15)                0        ['embedding[0][0]']        \n",
      "                                                                                        \n",
      " flatten_1 (Flatten)      (None, 3)                 0        ['embedding_1[0][0]']      \n",
      "                                                                                        \n",
      " batch_normalization_1 (  (None, 11)                44       ['num[0][0]']              \n",
      " BatchNormalization)                                                                    \n",
      "                                                                                        \n",
      " concatenate (Concatenat  (None, 18)                0        ['flatten[0][0]',          \n",
      " e)                                                           'flatten_1[0][0]']        \n",
      "                                                                                        \n",
      " dense_2 (Dense)          (None, 128)               1536     ['batch_normalization_1[0][\n",
      "                                                             0]']                       \n",
      "                                                                                        \n",
      " flatten_2 (Flatten)      (None, 18)                0        ['concatenate[0][0]']      \n",
      "                                                                                        \n",
      " dropout_1 (Dropout)      (None, 128)               0        ['dense_2[0][0]']          \n",
      "                                                                                        \n",
      " dense (Dense)            (None, 32)                608      ['flatten_2[0][0]']        \n",
      "                                                                                        \n",
      " batch_normalization_2 (  (None, 128)               512      ['dropout_1[0][0]']        \n",
      " BatchNormalization)                                                                    \n",
      "                                                                                        \n",
      " dropout (Dropout)        (None, 32)                0        ['dense[0][0]']            \n",
      "                                                                                        \n",
      " dense_3 (Dense)          (None, 128)               16512    ['batch_normalization_2[0][\n",
      "                                                             0]']                       \n",
      "                                                                                        \n",
      " batch_normalization (Ba  (None, 32)                128      ['dropout[0][0]']          \n",
      " tchNormalization)                                                                      \n",
      "                                                                                        \n",
      " dense_4 (Dense)          (None, 64)                8256     ['dense_3[0][0]']          \n",
      "                                                                                        \n",
      " dense_1 (Dense)          (None, 32)                1056     ['batch_normalization[0][0]\n",
      "                                                             ']                         \n",
      "                                                                                        \n",
      " concatenate_1 (Concaten  (None, 96)                0        ['dense_4[0][0]',          \n",
      " ate)                                                         'dense_1[0][0]']          \n",
      "                                                                                        \n",
      " dense_5 (Dense)          (None, 64)                6208     ['concatenate_1[0][0]']    \n",
      "                                                                                        \n",
      " dense_6 (Dense)          (None, 1)                 65       ['dense_5[0][0]']          \n",
      "                                                                                        \n",
      "========================================================================================\n",
      "Total params: 37952 (148.25 KB)\n",
      "Trainable params: 37610 (146.91 KB)\n",
      "Non-trainable params: 342 (1.34 KB)\n",
      "________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:14:23.182716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.185385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.185479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.186746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.186809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.186848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.229163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.229246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.229296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-22 20:14:23.229344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7665 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Concatenate, Embedding, Flatten, Reshape, LSTM, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import mean_absolute_error\n",
    "import math\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "categorical_inputs = []\n",
    "for cat in cat_cols:\n",
    "    categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "\n",
    "categorical_embeddings = []\n",
    "for i, cat in enumerate(cat_cols):\n",
    "    categorical_embeddings.append(Embedding(embed_sizes[i], int(math.sqrt(embed_sizes[i]))+1)(categorical_inputs[i]))\n",
    "\n",
    "concatenated = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\n",
    "categorical_logits = Flatten()(concatenated)\n",
    "categorical_logits = Dense(32,activation='relu')(categorical_logits)\n",
    "categorical_logits =Dropout(0.25)(categorical_logits)\n",
    "categorical_logits =BatchNormalization()(categorical_logits)\n",
    "categorical_logits = Dense(32,activation='relu')(categorical_logits)\n",
    "\n",
    "\n",
    "# Input for numerical data\n",
    "numerical_inputs = Input(shape=(11,), name='num')\n",
    "numerical_logits = numerical_inputs\n",
    "numerical_logits = BatchNormalization()(numerical_logits)\n",
    "\n",
    "numerical_logits = Dense(128,activation='relu')(numerical_logits)\n",
    "numerical_logits = Dropout(0.25)(numerical_logits)\n",
    "numerical_logits = BatchNormalization()(numerical_logits)\n",
    "numerical_logits = Dense(128,activation='relu')(numerical_logits)\n",
    "numerical_logits = Dense(64,activation='relu')(numerical_logits)\n",
    "\n",
    "\n",
    "logits = Concatenate()([numerical_logits,categorical_logits])\n",
    "logits = Dense(64,activation='relu')(logits)\n",
    "out = Dense(1, activation='linear')(logits)\n",
    "\n",
    "model = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\n",
    "model.compile(optimizer='adam',loss=mean_absolute_error)\n",
    "\n",
    "model.summary(line_length=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2f2b37-5e35-4c96-9e03-e8ee9a1c7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, valid_idx = next(gss.split(train, groups=train['date_id']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab2c917-2ba7-4d75-bfbf-a0259a6e27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/guowenrui/market-nn-if-you-like-you-can-use-it-and-upvote\n",
    "class SWA(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, filepath, swa_epoch):\n",
    "        super(SWA, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.swa_epoch = swa_epoch \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        print('Stochastic weight averaging selected for last {} epochs.'\n",
    "              .format(self.nb_epoch - self.swa_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == self.swa_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            \n",
    "        elif epoch > self.swa_epoch:    \n",
    "            for i in range(len(self.swa_weights)):\n",
    "                self.swa_weights[i] = (self.swa_weights[i] * \n",
    "                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "        print('Final model parameters set to stochastic weight average.')\n",
    "        self.model.save_weights(self.filepath)\n",
    "        print('Final stochastic averaged weights saved to file.')\n",
    "        \n",
    "class SnapshotCallbackBuilder:\n",
    "    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.15):\n",
    "        self.T = nb_epochs\n",
    "        self.M = nb_snapshots\n",
    "        self.alpha_zero = init_lr\n",
    "\n",
    "    def get_callbacks(self, model_prefix='Model'):\n",
    "\n",
    "        callback_list = [\n",
    "            callbacks.ModelCheckpoint(\"model.hdf5\",monitor='val_my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=500),\n",
    "            swa,\n",
    "            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n",
    "        ]\n",
    "\n",
    "        return callback_list\n",
    "\n",
    "    def _cosine_anneal_schedule(self, t):\n",
    "        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n",
    "        cos_inner /= self.T // self.M\n",
    "        cos_out = np.cos(cos_inner) + 1\n",
    "        return float(self.alpha_zero / 2 * cos_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f3e88f-cadb-452c-bb24-4226a3c66926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, valid_idx = next(gss.split(X, groups=X['date_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ed8f3f-4a03-41e8-b9c9-7a44acc708ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(market_train, market_target, indices):\n",
    "    X_num = market_train[num_cols].iloc[indices].values\n",
    "    X = {'num':X_num}\n",
    "    for cat in cat_cols:\n",
    "        X[cat] = market_train[cat].iloc[indices].values\n",
    "    y = market_target.iloc[indices].values\n",
    "    bid_size = market_train['bid_size_not_norm'].iloc[indices]\n",
    "    ask_size = market_train['ask_size_not_norm'].iloc[indices]\n",
    "\n",
    "    return X,y, bid_size, ask_size\n",
    "\n",
    "# r, u and d are used to calculate the scoring metric\n",
    "X_train,y_train, bid_size_train, ask_size_train = get_input(X, y, train_idx)\n",
    "X_valid,y_valid, bid_size_valid, ask_size_valid = get_input(X,y, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b60435-69f5-44a6-8900-f763d5a16d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic weight averaging selected for last 24 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:14:25.317136: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6034662220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-22 20:14:25.317150: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2023-11-22 20:14:25.319518: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-22 20:14:25.325916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-22 20:14:25.367332: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 6.08239, saving model to model.hdf5\n",
      "\n",
      "Epoch 2: val_loss improved from 6.08239 to 6.07676, saving model to model.hdf5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 6.07676\n",
      "\n",
      "Epoch 4: val_loss improved from 6.07676 to 6.07341, saving model to model.hdf5\n",
      "\n",
      "Epoch 5: val_loss improved from 6.07341 to 6.06627, saving model to model.hdf5\n",
      "\n",
      "Epoch 6: val_loss improved from 6.06627 to 6.06534, saving model to model.hdf5\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 7: val_loss improved from 6.06534 to 6.06047, saving model to model.hdf5\n",
      "\n",
      "Epoch 8: val_loss improved from 6.06047 to 6.05962, saving model to model.hdf5\n",
      "\n",
      "Epoch 9: val_loss did not improve from 6.05962\n",
      "\n",
      "Epoch 10: val_loss improved from 6.05962 to 6.05915, saving model to model.hdf5\n",
      "\n",
      "Epoch 11: val_loss improved from 6.05915 to 6.05896, saving model to model.hdf5\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: val_loss improved from 6.05896 to 6.05654, saving model to model.hdf5\n",
      "\n",
      "Epoch 13: val_loss did not improve from 6.05654\n",
      "\n",
      "Epoch 14: val_loss did not improve from 6.05654\n",
      "\n",
      "Epoch 15: val_loss did not improve from 6.05654\n",
      "\n",
      "Epoch 16: val_loss improved from 6.05654 to 6.05624, saving model to model.hdf5\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 17: val_loss improved from 6.05624 to 6.05566, saving model to model.hdf5\n",
      "\n",
      "Epoch 18: val_loss did not improve from 6.05566\n",
      "\n",
      "Epoch 19: val_loss did not improve from 6.05566\n",
      "\n",
      "Epoch 20: val_loss improved from 6.05566 to 6.05462, saving model to model.hdf5\n",
      "\n",
      "Epoch 21: val_loss did not improve from 6.05462\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 6.05462\n",
      "\n",
      "Epoch 23: val_loss improved from 6.05462 to 6.05452, saving model to model.hdf5\n",
      "\n",
      "Epoch 24: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 25: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 26: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 27: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 28: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 29: val_loss did not improve from 6.05452\n",
      "\n",
      "Epoch 30: val_loss did not improve from 6.05452\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "if train_model:\n",
    "    check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n",
    "    early_stop = EarlyStopping(patience=15,verbose=True)\n",
    "    reduce_lr = ReduceLROnPlateau( mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "    swa = SWA('model_swa.hdf5',6)\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_valid,y_valid),\n",
    "                  batch_size=1000,\n",
    "                  epochs=30,\n",
    "                  verbose=False,\n",
    "                  callbacks=[early_stop,check_point, reduce_lr, swa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aaac485-95a5-40aa-8e0b-3175fb2589c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "    \n",
    "y_min, y_max = -64, 64\n",
    "if train_model:\n",
    "    model.load_weights('model.hdf5')\n",
    "else:\n",
    "    model.load_weights(model_to_restore)\n",
    "    \n",
    "model.load_weights('model_swa.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae211399-c032-4dcb-91a8-f6f976a7e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32994/32994 [==============================] - 19s 566us/step\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test set\n",
    "predictions = model.predict(X_valid)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aed6527-96a1-4700-b112-97399c1aff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2, ..., 189, 190, 196])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid['stock_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4145311c-873b-4593-8e68-771afd866756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.0543914551045726\n",
      "Mean Relative Error: 2594223027671.914\n",
      "Mean Absolute Error CP: 6.053992847317898\n",
      "Mean Relative Error CP: 2591382232413.9473\n"
     ]
    }
   ],
   "source": [
    "predictions_to_clip = zero_sum(predictions, bid_size_valid + ask_size_valid)\n",
    "\n",
    "min_values = np.array([stock_id_ranges[stock_id][0] for stock_id in X_valid['stock_id']])\n",
    "max_values = np.array([stock_id_ranges[stock_id][1] for stock_id in X_valid['stock_id']])\n",
    "    \n",
    "clipped_predictions = np.clip(predictions_to_clip, min_values, max_values)\n",
    "\n",
    "\n",
    "# Calculate the mean absolute and squared error\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_valid, predictions))\n",
    "print(\"Mean Relative Error:\", mean_absolute_percentage_error(y_valid, predictions))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Mean Absolute Error CP:\", mean_absolute_error(y_valid, clipped_predictions))\n",
    "print(\"Mean Relative Error CP:\", mean_absolute_percentage_error(y_valid, clipped_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dad782cb-1969-4c97-9f3a-66ed5ab6c727",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optiver2023'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptiver2023\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m optiver2023\u001b[38;5;241m.\u001b[39mmake_env()\n\u001b[1;32m      3\u001b[0m iter_test \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39miter_test()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optiver2023'"
     ]
    }
   ],
   "source": [
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TRAIN_TARGET = \"target\"\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "    \n",
    "y_min, y_max = -64, 64\n",
    "\n",
    "# Making predictions on the test set\n",
    "\n",
    "counter = 0\n",
    "for (df_test, revealed_targets, sample_prediction) in tqdm(iter_test):\n",
    "#     display(df_test); display(revealed_targets); display(sample_prediction)\n",
    "    test_nan_ = df_test.isnull().sum()\n",
    "    df_test['far_price'].fillna(0, inplace=True)\n",
    "    df_test['near_price'].fillna(1, inplace=True)\n",
    "    \n",
    "    df_test['bid_size_not_norm']=df_test['bid_size']\n",
    "    df_test['ask_size_not_norm']=df_test['ask_size']\n",
    "    \n",
    "    df_test[cols_fill_nan] = df_test[cols_fill_nan].fillna(train_grouped_median)\n",
    "    test_nan = pd.DataFrame(dict(before=test_nan_, after=df_test.isnull().sum()))\n",
    "\n",
    "    df_test[num_cols] = df_test[num_cols].astype('float32')\n",
    "\n",
    "    df_test[num_cols] = robust_scaler.fit_transform(df_test[num_cols])\n",
    "    \n",
    "    X = {'num':df_test[num_cols].values}\n",
    "    for i, column in enumerate(cat_cols):\n",
    "        X[column] = df_test[column].astype(str).apply(lambda x: encode(encoders[i], x)).values\n",
    "\n",
    "    #X_test = scaler.transform(X_test)\n",
    "\n",
    "    df_test[TRAIN_TARGET] = model.predict(X)[:,0]\n",
    "    if counter < 5:\n",
    "        display(df_test.head())\n",
    "        display(test_nan.T)\n",
    "\n",
    "    preds = zero_sum(df_test[TRAIN_TARGET], df_test['bid_size_not_norm'] + df_test['ask_size_not_norm'])\n",
    "    clipped_predictions = np.clip(preds, y_min, y_max)\n",
    "    sample_prediction['target'] = clipped_predictions\n",
    "    prediction = sample_prediction\n",
    "    env.predict(prediction)\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
